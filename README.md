# reddit_kafka_spark





## Project Overview

This project streams data from a Kafka topic to a MongoDB database using Apache Spark. The streaming data is ingested, processed, and stored in MongoDB, where it is verified and 
accessed for analytics or downstream applications. Docker Compose is used to manage and deploy all the required services in a streamlined manner

## Step 1: Prerequisites

Tools Required:

Docker and Docker Compose

Apache Kafka and Confluent components

Apache Spark

MongoDB

MongoDB Atlas or a local MongoDB instance

VSCode or a similar IDE for development and verification

## Configurations Needed:

MongoDB Atlas URI (or local MongoDB connection string)

Kafka topic name and bootstrap server

Spark dependencies for Kafka and MongoDB connectors

Python development environment with PySpark installed
